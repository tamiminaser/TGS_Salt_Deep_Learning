{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout\n",
    "from tqdm import tqdm_notebook\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "#pd.set_option('display.max_colwidth',50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3969, 4) (18000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_Image_path</th>\n",
       "      <th>Train_Mask_path</th>\n",
       "      <th>id</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./train/images/44381a3f55.png</td>\n",
       "      <td>./train/masks/44381a3f55.png</td>\n",
       "      <td>44381a3f55</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./train/images/70db04a203.png</td>\n",
       "      <td>./train/masks/70db04a203.png</td>\n",
       "      <td>70db04a203</td>\n",
       "      <td>776.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./train/images/42c563d895.png</td>\n",
       "      <td>./train/masks/42c563d895.png</td>\n",
       "      <td>42c563d895</td>\n",
       "      <td>505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./train/images/d422a9eb8f.png</td>\n",
       "      <td>./train/masks/d422a9eb8f.png</td>\n",
       "      <td>d422a9eb8f</td>\n",
       "      <td>662.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./train/images/6d4fca6a35.png</td>\n",
       "      <td>./train/masks/6d4fca6a35.png</td>\n",
       "      <td>6d4fca6a35</td>\n",
       "      <td>443.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Train_Image_path               Train_Mask_path          id  \\\n",
       "0  ./train/images/44381a3f55.png  ./train/masks/44381a3f55.png  44381a3f55   \n",
       "1  ./train/images/70db04a203.png  ./train/masks/70db04a203.png  70db04a203   \n",
       "2  ./train/images/42c563d895.png  ./train/masks/42c563d895.png  42c563d895   \n",
       "3  ./train/images/d422a9eb8f.png  ./train/masks/d422a9eb8f.png  d422a9eb8f   \n",
       "4  ./train/images/6d4fca6a35.png  ./train/masks/6d4fca6a35.png  6d4fca6a35   \n",
       "\n",
       "       z  \n",
       "0   73.0  \n",
       "1  776.0  \n",
       "2  505.0  \n",
       "3  662.0  \n",
       "4  443.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Image_folder='./train/images/'\n",
    "Train_Mask_folder='./train/masks/'\n",
    "Test_Image_folder='./test/'\n",
    "Train_Image_name=os.listdir(path=Train_Image_folder)\n",
    "Test_Image_name=os.listdir(path=Test_Image_folder)\n",
    "Train_Image_path=[]\n",
    "Train_Mask_path=[]\n",
    "Train_id=[]\n",
    "for i in Train_Image_name:\n",
    "    path1=Train_Image_folder+i\n",
    "    path2=Train_Mask_folder+i\n",
    "    id1=i.split(sep='.')[0]\n",
    "    Train_Image_path.append(path1)\n",
    "    Train_Mask_path.append(path2)\n",
    "    Train_id.append(id1)\n",
    "  \n",
    "\n",
    "Test_Image_path=[]\n",
    "Test_id=[]\n",
    "for i in Test_Image_name:\n",
    "    path=Test_Image_folder+i\n",
    "    id2=i.split(sep='.')[0]\n",
    "    Test_Image_path.append(path)\n",
    "    Test_id.append(id2)\n",
    "    \n",
    "df_Train_path=pd.DataFrame({'id':Train_id,'Train_Image_path':Train_Image_path,'Train_Mask_path':Train_Mask_path})\n",
    "df_Test_path=pd.DataFrame({'id':Test_id,'Test_Image_path':Test_Image_path})\n",
    "\n",
    "df_depths=pd.read_csv('./depths.csv')\n",
    "df_sub=pd.read_csv('./sample_submission.csv')\n",
    "df_Train_path=df_Train_path.merge(df_depths,on='id',how='left')\n",
    "df_Test_path=df_Test_path.merge(df_depths,on='id',how='left')\n",
    "df_Test_path=df_sub.merge(df_Test_path,on='id',how='left')\n",
    "print(df_Train_path.shape,df_Test_path.shape)\n",
    "df_Train_path.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Test_Image_path</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>155410d6fa</td>\n",
       "      <td>./test/155410d6fa.png</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78b32781d1</td>\n",
       "      <td>./test/78b32781d1.png</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63db2a476a</td>\n",
       "      <td>./test/63db2a476a.png</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17bfcdb967</td>\n",
       "      <td>./test/17bfcdb967.png</td>\n",
       "      <td>698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7ea0fd3c88</td>\n",
       "      <td>./test/7ea0fd3c88.png</td>\n",
       "      <td>837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        Test_Image_path    z\n",
       "0  155410d6fa  ./test/155410d6fa.png  559\n",
       "1  78b32781d1  ./test/78b32781d1.png  298\n",
       "2  63db2a476a  ./test/63db2a476a.png  392\n",
       "3  17bfcdb967  ./test/17bfcdb967.png  698\n",
       "4  7ea0fd3c88  ./test/7ea0fd3c88.png  837"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test_path.drop('rle_mask',axis=1,inplace=True)\n",
    "df_Test_path.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d370e8adf124f5fa596b677ccdf4fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3969), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tamiminaser/anaconda/lib/python3.6/site-packages/keras_preprocessing/image.py:489: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "cannot identify image file './train/images/.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-edb183f85dbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_Train_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_Train_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain_Image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_Train_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"masks\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_Train_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain_Mask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_Train_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"coverage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_Train_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcov_to_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-edb183f85dbd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_Train_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_Train_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain_Image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_Train_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"masks\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_Train_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain_Mask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_Train_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"coverage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_Train_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcov_to_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tamiminaser/anaconda/lib/python3.6/site-packages/keras_preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    493\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    494\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[0;32m--> 495\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tamiminaser/anaconda/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2583\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m     raise IOError(\"cannot identify image file %r\"\n\u001b[0;32m-> 2585\u001b[0;31m                   % (filename if filename else fp))\n\u001b[0m\u001b[1;32m   2586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: cannot identify image file './train/images/.DS_Store'"
     ]
    }
   ],
   "source": [
    "df_Train_path[\"images\"] = [np.array(load_img(path=idx, grayscale=True)) / 255 for idx in tqdm_notebook(df_Train_path.Train_Image_path)]\n",
    "df_Train_path[\"masks\"]=[np.array(load_img(path=idx, grayscale=True)) / 255 for idx in tqdm_notebook(df_Train_path.Train_Mask_path)]\n",
    "df_Train_path[\"coverage\"] = df_Train_path.masks.map(np.sum) / pow(101, 2)\n",
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "        \n",
    "df_Train_path[\"coverage_class\"] = df_Train_path.coverage.map(cov_to_class)\n",
    "#df_Train_path.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size_ori = 101\n",
    "img_size_target = 128\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "\n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n",
    "    df_Train_path.id.values,\n",
    "    np.array(df_Train_path.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    np.array(df_Train_path.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    df_Train_path.coverage.values,\n",
    "    df_Train_path.z.values,\n",
    "    test_size=0.2, stratify=df_Train_path.coverage_class, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200,) (800,)\n",
      "(3200, 128, 128, 1) (3200, 128, 128, 1)\n",
      "(800, 128, 128, 1) (800, 128, 128, 1)\n",
      "(3200,) (800,)\n",
      "(3200,) (800,)\n"
     ]
    }
   ],
   "source": [
    "print(ids_train.shape,ids_valid.shape)\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_valid.shape,y_valid.shape)\n",
    "print(cov_train.shape,cov_test.shape)\n",
    "print(depth_train.shape,depth_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define IoU metric\n",
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another method\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + K.epsilon()) / (K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_layer, start_neurons):\n",
    "    # 128 -> 64\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    # 64 -> 32\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(0.5)(pool2)\n",
    "\n",
    "    # 32 -> 16\n",
    "    conv3 = Conv2D(start_neurons * 4, (5, 5), activation=\"relu\", padding=\"same\")(pool2)\n",
    "    conv3 = Conv2D(start_neurons * 4, (5, 5), activation=\"relu\", padding=\"same\")(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(0.5)(pool3)\n",
    "\n",
    "    # 16 -> 8\n",
    "    conv4 = Conv2D(start_neurons * 8, (5, 5), activation=\"relu\", padding=\"same\")(pool3)\n",
    "    conv4 = Conv2D(start_neurons * 8, (5, 5), activation=\"relu\", padding=\"same\")(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(0.5)(pool4)\n",
    "\n",
    "    # Middle\n",
    "    convm = Conv2D(start_neurons * 16, (5, 5), activation=\"relu\", padding=\"same\")(pool4)\n",
    "    convm = Conv2D(start_neurons * 16, (5, 5), activation=\"relu\", padding=\"same\")(convm)\n",
    "\n",
    "    # 8 -> 16\n",
    "    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = Dropout(0.5)(uconv4)\n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "\n",
    "    # 16 -> 32\n",
    "    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "    uconv3 = concatenate([deconv3, conv3])\n",
    "    uconv3 = Dropout(0.5)(uconv3)\n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "\n",
    "    # 32 -> 64\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Dropout(0.5)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "\n",
    "    # 64 -> 128\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3,3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Dropout(0.5)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3,3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3,3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "\n",
    "    uncov1 = Dropout(0.5)(uconv1)\n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n",
    "    \n",
    "    return output_layer\n",
    "\n",
    "input_layer = Input((img_size_target, img_size_target, 1))\n",
    "output_layer = build_model(input_layer, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(input_layer, output_layer)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.001), metrics=[mean_iou])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n",
    "y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6400 samples, validate on 800 samples\n",
      "Epoch 1/80\n",
      "6400/6400 [==============================] - 1190s 186ms/step - loss: 0.5179 - mean_iou: 0.3818 - val_loss: 0.4427 - val_mean_iou: 0.3832\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44266, saving model to ./keras.model\n",
      "Epoch 2/80\n",
      "6400/6400 [==============================] - 1042s 163ms/step - loss: 0.4902 - mean_iou: 0.3907 - val_loss: 0.3808 - val_mean_iou: 0.4036\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.44266 to 0.38081, saving model to ./keras.model\n",
      "Epoch 3/80\n",
      "6400/6400 [==============================] - 1047s 164ms/step - loss: 0.3794 - mean_iou: 0.4270 - val_loss: 0.3868 - val_mean_iou: 0.4475\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/80\n",
      "6400/6400 [==============================] - 1114s 174ms/step - loss: 0.3435 - mean_iou: 0.4621 - val_loss: 0.2885 - val_mean_iou: 0.4794\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.38081 to 0.28855, saving model to ./keras.model\n",
      "Epoch 5/80\n",
      "6400/6400 [==============================] - 1122s 175ms/step - loss: 0.2940 - mean_iou: 0.4999 - val_loss: 0.2807 - val_mean_iou: 0.5160\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.28855 to 0.28072, saving model to ./keras.model\n",
      "Epoch 6/80\n",
      "6400/6400 [==============================] - 1159s 181ms/step - loss: 0.2737 - mean_iou: 0.5311 - val_loss: 0.2664 - val_mean_iou: 0.5447\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.28072 to 0.26635, saving model to ./keras.model\n",
      "Epoch 7/80\n",
      "6400/6400 [==============================] - 1067s 167ms/step - loss: 0.2646 - mean_iou: 0.5568 - val_loss: 0.2722 - val_mean_iou: 0.5670\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/80\n",
      "6400/6400 [==============================] - 1058s 165ms/step - loss: 0.2379 - mean_iou: 0.5775 - val_loss: 0.2334 - val_mean_iou: 0.5874\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.26635 to 0.23344, saving model to ./keras.model\n",
      "Epoch 9/80\n",
      "6400/6400 [==============================] - 1061s 166ms/step - loss: 0.2184 - mean_iou: 0.5970 - val_loss: 0.2224 - val_mean_iou: 0.6057\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.23344 to 0.22236, saving model to ./keras.model\n",
      "Epoch 10/80\n",
      "6400/6400 [==============================] - 1061s 166ms/step - loss: 0.2117 - mean_iou: 0.6139 - val_loss: 0.2120 - val_mean_iou: 0.6212\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.22236 to 0.21201, saving model to ./keras.model\n",
      "Epoch 11/80\n",
      "6400/6400 [==============================] - 1060s 166ms/step - loss: 0.1984 - mean_iou: 0.6281 - val_loss: 0.2292 - val_mean_iou: 0.6348\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/80\n",
      "6400/6400 [==============================] - 1054s 165ms/step - loss: 0.1938 - mean_iou: 0.6411 - val_loss: 0.2150 - val_mean_iou: 0.6469\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/80\n",
      "6400/6400 [==============================] - 1056s 165ms/step - loss: 0.1900 - mean_iou: 0.6524 - val_loss: 0.2133 - val_mean_iou: 0.6574\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/80\n",
      "6400/6400 [==============================] - 1058s 165ms/step - loss: 0.1811 - mean_iou: 0.6626 - val_loss: 0.2023 - val_mean_iou: 0.6670\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.21201 to 0.20228, saving model to ./keras.model\n",
      "Epoch 15/80\n",
      "6400/6400 [==============================] - 1057s 165ms/step - loss: 0.1824 - mean_iou: 0.6712 - val_loss: 0.2106 - val_mean_iou: 0.6752\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/80\n",
      "6400/6400 [==============================] - 1062s 166ms/step - loss: 0.1794 - mean_iou: 0.6791 - val_loss: 0.2007 - val_mean_iou: 0.6825\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.20228 to 0.20071, saving model to ./keras.model\n",
      "Epoch 17/80\n",
      "6400/6400 [==============================] - 1061s 166ms/step - loss: 0.1786 - mean_iou: 0.6861 - val_loss: 0.1837 - val_mean_iou: 0.6892\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.20071 to 0.18367, saving model to ./keras.model\n",
      "Epoch 18/80\n",
      "6400/6400 [==============================] - 1047s 164ms/step - loss: 0.1622 - mean_iou: 0.6926 - val_loss: 0.2037 - val_mean_iou: 0.6958\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/80\n",
      "6400/6400 [==============================] - 1048s 164ms/step - loss: 0.1596 - mean_iou: 0.6990 - val_loss: 0.2234 - val_mean_iou: 0.7019\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/80\n",
      "6400/6400 [==============================] - 1048s 164ms/step - loss: 0.1580 - mean_iou: 0.7047 - val_loss: 0.1696 - val_mean_iou: 0.7074\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.18367 to 0.16964, saving model to ./keras.model\n",
      "Epoch 21/80\n",
      "6400/6400 [==============================] - 1049s 164ms/step - loss: 0.1505 - mean_iou: 0.7100 - val_loss: 0.1857 - val_mean_iou: 0.7128\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/80\n",
      "6400/6400 [==============================] - 1053s 165ms/step - loss: 0.1526 - mean_iou: 0.7154 - val_loss: 0.1986 - val_mean_iou: 0.7178\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/80\n",
      "6400/6400 [==============================] - 1050s 164ms/step - loss: 0.1580 - mean_iou: 0.7201 - val_loss: 0.2133 - val_mean_iou: 0.7217\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/80\n",
      "6400/6400 [==============================] - 1053s 164ms/step - loss: 0.1562 - mean_iou: 0.7233 - val_loss: 0.1805 - val_mean_iou: 0.7253\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/80\n",
      "6400/6400 [==============================] - 1053s 165ms/step - loss: 0.1393 - mean_iou: 0.7274 - val_loss: 0.1922 - val_mean_iou: 0.7295\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/80\n",
      "6400/6400 [==============================] - 1054s 165ms/step - loss: 0.1351 - mean_iou: 0.7316 - val_loss: 0.1992 - val_mean_iou: 0.7335\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 27/80\n",
      "6400/6400 [==============================] - 1047s 164ms/step - loss: 0.1154 - mean_iou: 0.7358 - val_loss: 0.1682 - val_mean_iou: 0.7379\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.16964 to 0.16820, saving model to ./keras.model\n",
      "Epoch 28/80\n",
      "6400/6400 [==============================] - 1053s 165ms/step - loss: 0.1084 - mean_iou: 0.7401 - val_loss: 0.1777 - val_mean_iou: 0.7422\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/80\n",
      "6400/6400 [==============================] - 1052s 164ms/step - loss: 0.1062 - mean_iou: 0.7443 - val_loss: 0.1767 - val_mean_iou: 0.7463\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/80\n",
      "6400/6400 [==============================] - 1054s 165ms/step - loss: 0.1045 - mean_iou: 0.7483 - val_loss: 0.1757 - val_mean_iou: 0.7501\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/80\n",
      "6400/6400 [==============================] - 1053s 165ms/step - loss: 0.1015 - mean_iou: 0.7520 - val_loss: 0.1776 - val_mean_iou: 0.7538\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/80\n",
      "6400/6400 [==============================] - 1053s 164ms/step - loss: 0.1030 - mean_iou: 0.7557 - val_loss: 0.1828 - val_mean_iou: 0.7573\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/80\n",
      "6400/6400 [==============================] - 1051s 164ms/step - loss: 0.0987 - mean_iou: 0.7590 - val_loss: 0.1899 - val_mean_iou: 0.7607\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 34/80\n",
      "6400/6400 [==============================] - 1054s 165ms/step - loss: 0.0958 - mean_iou: 0.7624 - val_loss: 0.1842 - val_mean_iou: 0.7640\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/80\n",
      "6400/6400 [==============================] - 1030s 161ms/step - loss: 0.0970 - mean_iou: 0.7656 - val_loss: 0.1854 - val_mean_iou: 0.7670\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/80\n",
      "6400/6400 [==============================] - 34185s 5s/step - loss: 0.0975 - mean_iou: 0.7685 - val_loss: 0.1830 - val_mean_iou: 0.7700\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/80\n",
      "6400/6400 [==============================] - 1025s 160ms/step - loss: 0.0976 - mean_iou: 0.7713 - val_loss: 0.1842 - val_mean_iou: 0.7727\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 00037: early stopping\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience=10, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(\"./keras.model\", save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n",
    "\n",
    "epochs = 80\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\"./keras.model\",custom_objects={'mean_iou': mean_iou})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\n",
    "preds_valid = np.array([downsample(x) for x in preds_valid])\n",
    "y_valid = np.array([downsample(x) for x in y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iou_metric(y_true_in, y_pred_in, print_table=False):\n",
    "    labels = y_true_in\n",
    "    y_pred = y_pred_in\n",
    "    \n",
    "    true_objects = 2\n",
    "    pred_objects = 2\n",
    "\n",
    "    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins = true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    # Exclude background from the analysis\n",
    "    intersection = intersection[1:,1:]\n",
    "    union = union[1:,1:]\n",
    "    union[union == 0] = 1e-9\n",
    "\n",
    "    # Compute the intersection over union\n",
    "    iou = intersection / union\n",
    "\n",
    "    # Precision helper function\n",
    "    def precision_at(threshold, iou):\n",
    "        matches = iou > threshold\n",
    "        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n",
    "        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "        return tp, fp, fn\n",
    "\n",
    "    # Loop over IoU thresholds\n",
    "    prec = []\n",
    "    if print_table:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, iou)\n",
    "        if (tp + fp + fn) > 0:\n",
    "            p = tp / (tp + fp + fn)\n",
    "        else:\n",
    "            p = 0\n",
    "        if print_table:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n",
    "        prec.append(p)\n",
    "    \n",
    "    if print_table:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "    return np.mean(prec)\n",
    "\n",
    "def iou_metric_batch(y_true_in, y_pred_in):\n",
    "    batch_size = y_true_in.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n",
    "        metric.append(value)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b2a11a675c49a68f617d27b8d82446"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "thresholds = np.linspace(0, 1, 50)\n",
    "ious = np.array([iou_metric_batch(y_valid, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold:  0.65306122449\n"
     ]
    }
   ],
   "source": [
    "threshold_best_index = np.argmax(ious[9:-10]) + 9\n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]\n",
    "print('Best Threshold: ',threshold_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#threshold_best=6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source https://www.kaggle.com/bguberfain/unet-with-depth\n",
    "def RLenc(img, order='F', format=True):\n",
    "    \"\"\"\n",
    "    img is binary mask image, shape (r,c)\n",
    "    order is down-then-right, i.e. Fortran\n",
    "    format determines if the order needs to be preformatted (according to submission rules) or not\n",
    "\n",
    "    returns run length as an array or string (if format is True)\n",
    "    \"\"\"\n",
    "    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n",
    "    runs = []  ## list of run lengths\n",
    "    r = 0  ## the current run length\n",
    "    pos = 1  ## count starts from 1 per WK\n",
    "    for c in bytes:\n",
    "        if (c == 0):\n",
    "            if r != 0:\n",
    "                runs.append((pos, r))\n",
    "                pos += r\n",
    "                r = 0\n",
    "            pos += 1\n",
    "        else:\n",
    "            r += 1\n",
    "\n",
    "    # if last run is unsaved (i.e. data ends with 1)\n",
    "    if r != 0:\n",
    "        runs.append((pos, r))\n",
    "        pos += r\n",
    "        r = 0\n",
    "\n",
    "    if format:\n",
    "        z = ''\n",
    "\n",
    "        for rr in runs:\n",
    "            z += '{} {} '.format(rr[0], rr[1])\n",
    "        return z[:-1]\n",
    "    else:\n",
    "        return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7928c0ecaa4d999320662f2cdaada8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test = np.array([upsample(np.array(load_img(path=idx, grayscale=True))) / 255 for idx in tqdm_notebook(df_Test_path.Test_Image_path)]).reshape(-1, img_size_target, img_size_target, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds_test = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5608db404bb84049bd1c4a5415a6e801"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(df_Test_path.id.values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\n",
    "sub.index.names = ['id']\n",
    "sub.columns = ['rle_mask']\n",
    "sub.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rle_mask</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155410d6fa</th>\n",
       "      <td>1 908 910 98 1011 96 1112 95 1213 94 1314 94 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78b32781d1</th>\n",
       "      <td>60 42 160 43 260 44 359 46 459 47 559 48 658 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63db2a476a</th>\n",
       "      <td>6057 3 6157 5 6257 6 6356 8 6455 10 6555 11 66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17bfcdb967</th>\n",
       "      <td>4354 3 4445 19 4470 14 4546 49 4597 1 4600 1 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7ea0fd3c88</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     rle_mask\n",
       "id                                                           \n",
       "155410d6fa  1 908 910 98 1011 96 1112 95 1213 94 1314 94 1...\n",
       "78b32781d1  60 42 160 43 260 44 359 46 459 47 559 48 658 5...\n",
       "63db2a476a  6057 3 6157 5 6257 6 6356 8 6455 10 6555 11 66...\n",
       "17bfcdb967  4354 3 4445 19 4470 14 4546 49 4597 1 4600 1 4...\n",
       "7ea0fd3c88                                                   "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
